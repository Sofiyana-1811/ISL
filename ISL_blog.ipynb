{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indian Sign Language Recognition  \n",
    "**By:** Ipsita Jain (210102039)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Sign language is a vital communication tool for the deaf and hard-of-hearing community. In India, where linguistic diversity is vast, Indian Sign Language (ISL) remains underrepresented in AI/ML applications. This project aims to build a gesture recognition system for ISL using deep learning, fostering inclusivity and accessibility.\n",
    "\n",
    "**Why this topic?**\n",
    "\n",
    "- **Social Impact:** Automation of ISL recognition can bridge communication gaps in education, healthcare, and public services.\n",
    "- **Technical Challenge:** Gesture recognition involves complexities like background noise, lighting variations, and real-time processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Perspective: Multimodal Learning\n",
    "\n",
    "Recent advancements in multimodal learning have revolutionized gesture recognition:\n",
    "\n",
    "- **Early 2010s:** Handcrafted features (e.g., HOG, SIFT) combined with classifiers like SVMs.\n",
    "- **Mid-2010s:** CNNs dominated image-based tasks, but required large labeled datasets.\n",
    "- **2020s:** Transformer-based models (e.g., ViT, CLIP) enabled cross-modal learning (e.g., text + images).\n",
    "\n",
    "**Connection to this work:**  \n",
    "This project adopts a CNN-based approach, inspired by modern architectures like ResNet, but tailored for ISL’s unique gestures and limited dataset sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "1. **Data Limitations:** ISL datasets are scarce compared to ASL. Data augmentation (rotation, flipping) was critical for generalization.\n",
    "2. **Model Trade-offs:** Lightweight CNNs (e.g., MobileNet) outperformed deeper models due to faster inference and lower overfitting.\n",
    "3. **Real-World Gaps:** Accuracy dropped in cluttered backgrounds, highlighting the need for robust preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code & Experiments\n",
    "\n",
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (from `function.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    for idx, folder in enumerate(os.listdir(data_dir)):\n",
    "        label_map[idx] = folder\n",
    "        for file in os.listdir(os.path.join(data_dir, folder)):\n",
    "            img_path = os.path.join(data_dir, folder, file)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (64, 64))\n",
    "            images.append(img)\n",
    "            labels.append(idx)\n",
    "    images = np.array(images)\n",
    "    labels = to_categorical(labels)\n",
    "    return images, labels, label_map\n",
    "\n",
    "# Example usage:\n",
    "# X_train, y_train, label_map = load_data('data/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture (from `train.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))  # Adjust according to number of classes\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model (from `train.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train, y_train, X_test, y_test are prepared\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation (from `train.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time Prediction Demo (from `app.py`)\n",
    "\n",
    "*This is a simplified version for illustration. See `app.py` for the full webcam-based demo.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    img = cv2.resize(frame, (64, 64))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    # Display prediction on frame\n",
    "    cv2.putText(frame, label_map[predicted_class], (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    cv2.imshow('ISL Recognition', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflections\n",
    "\n",
    "### Surprises\n",
    "\n",
    "- Background subtraction alone wasn’t sufficient for reliable predictions.\n",
    "- Transfer learning (e.g., using pretrained VGG16) initially hurt performance due to domain mismatch.\n",
    "\n",
    "### Improvements\n",
    "\n",
    "1. **Dataset Expansion:** Collaborate with ISL communities to collect more diverse samples.\n",
    "2. **Multimodal Fusion:** Integrate pose estimation (e.g., MediaPipe) to isolate hand movements.\n",
    "3. **Edge Deployment:** Optimize the model with TensorFlow Lite for mobile use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. MediaPipe Hands: On-device Real-time Hand Tracking (https://arxiv.org/abs/2006.10214)\n",
    "2. Keras Documentation (https://keras.io/)\n",
    "3. Indian Sign Language Dataset (Kaggle) (https://www.kaggle.com/datasets/grassknoted/asl-alphabet)\n",
    "4. Visual Haystacks: Berkeley AI Research Blog (https://bair.berkeley.edu/blog/2024/07/20/visual-haystacks/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
